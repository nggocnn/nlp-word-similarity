{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import string\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecpath = '../word2vec/W2V_150.txt'\n",
    "visim = '../Datasets/ViSim-400'\n",
    "vicon = '../Datasets/ViCon-400'\n",
    "words = []\n",
    "vecs = []\n",
    "dim = None\n",
    "n_vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def batchify(x: np.array, y: np.array, batch_size):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    total_sample = x.shape[0]\n",
    "    n_sections = total_sample // batch_size\n",
    "    for bx, by in zip(np.split(x, n_sections), np.split(y, n_sections)):\n",
    "        yield bx, by\n",
    "\n",
    "\n",
    "def word2norm(a: str) -> str:\n",
    "    table = str.maketrans(dict.fromkeys(\n",
    "        string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "    new_s = a.translate(table)\n",
    "    return new_s\n",
    "\n",
    "def word2vec(a: str) -> np.array:\n",
    "    try:\n",
    "        a = word2norm(a)\n",
    "        i = words.index(a)\n",
    "        return vecs[i]\n",
    "    except:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "\n",
    "def cosine(a: np.array, b: np.array) -> float:\n",
    "    a = a / np.linalg.norm(a) if np.linalg.norm(a) != 0 else a\n",
    "    b = b / np.linalg.norm(b) if np.linalg.norm(b) != 0 else b\n",
    "    return a.dot(b)\n",
    "\n",
    "\n",
    "#  Dot\tProduct Distance, Euclidean Distance, Dice Distance, Jaccard Distance.\n",
    "\n",
    "\n",
    "def dot(a: np.array, b: np.array) -> float:\n",
    "    return a.dot(b)\n",
    "\n",
    "\n",
    "def euclid(a: np.array, b: np.array) -> float:\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "\n",
    "def dice(a: np.array, b: np.array) -> float:\n",
    "    return distance.dice(a, b)\n",
    "\n",
    "\n",
    "def jaccard(a: np.array, b: np.array) -> float:\n",
    "    return distance.jaccard(a, b, w=None)\n",
    "\n",
    "\n",
    "def sim(row, sim_f=cosine):\n",
    "    vec1 = word2vec(row.iloc[0])\n",
    "    vec2 = word2vec(row.iloc[1])\n",
    "    return sim_f(vec1, vec2)\n",
    "\n",
    "\n",
    "def is_oov(word) -> bool:\n",
    "    a = word2norm(word)\n",
    "    return a not in words\n",
    "\n",
    "\n",
    "def drop_oov(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    not_oov_index = df.apply(lambda row: not any([is_oov(row.iloc[i]) for i in [0, 1]]), axis=1).tolist()\n",
    "    return df[not_oov_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading ../word2vec/W2V_150.txt to variables: 77023it [00:04, 16298.09it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(word2vecpath, encoding='utf8') as f:\n",
    "    for line in tqdm(f, f\"loading {word2vecpath} to variables\"):\n",
    "        if not n_vocab:\n",
    "            n_vocab = int(line)\n",
    "        elif not dim:\n",
    "            dim = int(line)\n",
    "        else:\n",
    "            line = line.replace('\\n', '')\n",
    "            words.append(word2norm(line.split('  ')[0]))\n",
    "            vecs.append([float(i) for i in line.split('  ')[1].split()])\n",
    "vecs = np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpairs = pd.read_csv(visim + '/Visim-400.txt', sep=\"\\t\")\n",
    "simpairs = drop_oov(simpairs)\n",
    "\n",
    "npairs = pd.read_csv(vicon + '/400_verb_pairs.txt', sep=\"\\t\")\n",
    "vpairs = pd.read_csv(vicon + '/400_verb_pairs.txt', sep=\"\\t\")\n",
    "apairs = pd.read_csv(vicon + '/600_adj_pairs.txt', sep=\"\\t\")\n",
    "\n",
    "testsetdf = pd.concat([npairs, vpairs, apairs])[['Word1', 'Word2', 'Relation']].drop_duplicates()\n",
    "testsetdf = drop_oov(testsetdf)\n",
    "testsetdf = shuffle_df(testsetdf)\n",
    "\n",
    "train_records = [\n",
    "    {\n",
    "        'Word1': line.split(' ')[0].strip(),\n",
    "        'Word2': line.split(' ')[1].strip(),\n",
    "        'Relation': 'SYN'\n",
    "    } for line in open('../antonym-synonym set/Synonym_vietnamese.txt', encoding='utf8')\n",
    "]\n",
    "\n",
    "train_records.extend([\n",
    "    {\n",
    "        'Word1': line.split()[0].strip(),\n",
    "        'Word2': line.split()[1].strip(),\n",
    "        'Relation': 'ANT'\n",
    "    } for line in open('../antonym-synonym set/Antonym_vietnamese.txt', encoding='utf8')\n",
    "])\n",
    "\n",
    "trainsetdf = pd.DataFrame.from_records(train_records).drop_duplicates()\n",
    "trainsetdf = drop_oov(trainsetdf)\n",
    "trainsetdf = shuffle_df(trainsetdf)\n",
    "\n",
    "\n",
    "def flatten(row):\n",
    "    vec1 = word2vec(row.iloc[0])\n",
    "    vec2 = word2vec(row.iloc[1])\n",
    "    return np.array([vec1, vec2])\n",
    "\n",
    "\n",
    "train_x = np.array([i for i in trainsetdf.apply(flatten, axis=1)])\n",
    "train_y = trainsetdf.Relation.map({'ANT': 0, 'SYN': 1})\n",
    "\n",
    "test_x = np.array([i for i in testsetdf.apply(flatten, axis=1)])\n",
    "test_y = testsetdf.Relation.map({'ANT': 0, 'SYN': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thèm</td>\n",
       "      <td>thèm_thuồng</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chầu_chực</td>\n",
       "      <td>chờ_chực</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vẹo</td>\n",
       "      <td>xiêu_vẹo</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lất_phất</td>\n",
       "      <td>phất_phơ</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lừa_đảo</td>\n",
       "      <td>tỉnh_ngộ</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>động</td>\n",
       "      <td>đụng</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>leo</td>\n",
       "      <td>trèo</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>thò</td>\n",
       "      <td>thọc</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>bất_thần</td>\n",
       "      <td>chợt</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>tinh</td>\n",
       "      <td>toàn</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word1        Word2 Relation\n",
       "0          thèm  thèm_thuồng      SYN\n",
       "1     chầu_chực     chờ_chực      SYN\n",
       "2           vẹo     xiêu_vẹo      SYN\n",
       "3      lất_phất     phất_phơ      SYN\n",
       "4       lừa_đảo     tỉnh_ngộ      ANT\n",
       "...         ...          ...      ...\n",
       "7345       động         đụng      SYN\n",
       "7346        leo         trèo      SYN\n",
       "7347        thò         thọc      SYN\n",
       "7348   bất_thần         chợt      SYN\n",
       "7349       tinh         toàn      SYN\n",
       "\n",
       "[7350 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chờ_mong</td>\n",
       "      <td>mong_chờ</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>đứng_tuổi</td>\n",
       "      <td>luống_tuổi</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bán</td>\n",
       "      <td>mua</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tàn_ác</td>\n",
       "      <td>tàn_tệ</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bác_bỏ</td>\n",
       "      <td>chấp_nhận</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>mau</td>\n",
       "      <td>thưa</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>chóng</td>\n",
       "      <td>muộn</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>chua_cay</td>\n",
       "      <td>sâu_cay</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>ăn_năn</td>\n",
       "      <td>ân_hận</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>dị_thường</td>\n",
       "      <td>khác_thường</td>\n",
       "      <td>SYN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word1        Word2 Relation\n",
       "0     chờ_mong     mong_chờ      SYN\n",
       "1    đứng_tuổi   luống_tuổi      SYN\n",
       "2          bán          mua      ANT\n",
       "3       tàn_ác       tàn_tệ      SYN\n",
       "4       bác_bỏ    chấp_nhận      ANT\n",
       "..         ...          ...      ...\n",
       "841        mau         thưa      ANT\n",
       "842      chóng         muộn      ANT\n",
       "843   chua_cay      sâu_cay      SYN\n",
       "844     ăn_năn       ân_hận      SYN\n",
       "845  dị_thường  khác_thường      SYN\n",
       "\n",
       "[846 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape = (7350, 2, 150)\n",
      "train_y shape = (7350,)\n",
      "test_x shape = (846, 2, 150)\n",
      "test_y.shape = (846,)\n"
     ]
    }
   ],
   "source": [
    "print(f'train_x shape = {train_x.shape}\\n'\n",
    "      f'train_y shape = {train_y.shape}\\n'\n",
    "      f'test_x shape = {test_x.shape}\\n'\n",
    "      f'test_y.shape = {test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(835, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsetdf_values = trainsetdf.apply(lambda row: '-'.join(row.iloc), axis=1).tolist()\n",
    "actual_testdf = testsetdf[testsetdf.apply(lambda row: '-'.join(row.iloc) in trainsetdf_values, axis=1).tolist()]\n",
    "\n",
    "atest_x = np.array([i for i in actual_testdf.apply(flatten, axis=1)])\n",
    "atest_y = actual_testdf.Relation.map({'ANT': 0, 'SYN': 1})\n",
    "actual_testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>Sim1</th>\n",
       "      <th>Sim2</th>\n",
       "      <th>STD</th>\n",
       "      <th>sim-cosine</th>\n",
       "      <th>sim-dot</th>\n",
       "      <th>sim-euclid</th>\n",
       "      <th>sim-dice</th>\n",
       "      <th>sim-jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biến</td>\n",
       "      <td>ngập</td>\n",
       "      <td>V</td>\n",
       "      <td>3.13</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.004912</td>\n",
       "      <td>-1.493676</td>\n",
       "      <td>25.296228</td>\n",
       "      <td>0.808093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nhà_thi_đấu</td>\n",
       "      <td>nhà</td>\n",
       "      <td>N</td>\n",
       "      <td>3.07</td>\n",
       "      <td>5.12</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.082523</td>\n",
       "      <td>18.257401</td>\n",
       "      <td>22.118834</td>\n",
       "      <td>-3.074252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>động</td>\n",
       "      <td>tĩnh</td>\n",
       "      <td>V</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.277086</td>\n",
       "      <td>39.547434</td>\n",
       "      <td>14.640360</td>\n",
       "      <td>-24.627852</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khuyết</td>\n",
       "      <td>ưu</td>\n",
       "      <td>N</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.176799</td>\n",
       "      <td>40.841349</td>\n",
       "      <td>19.508880</td>\n",
       "      <td>-0.468402</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thủ_pháp</td>\n",
       "      <td>biện_pháp</td>\n",
       "      <td>N</td>\n",
       "      <td>4.13</td>\n",
       "      <td>6.88</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.402366</td>\n",
       "      <td>106.914893</td>\n",
       "      <td>17.831482</td>\n",
       "      <td>-2.735911</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>triều_đại</td>\n",
       "      <td>cổ_đại</td>\n",
       "      <td>N</td>\n",
       "      <td>3.67</td>\n",
       "      <td>6.12</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>76.308166</td>\n",
       "      <td>20.353694</td>\n",
       "      <td>-6.902056</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>lình_xình</td>\n",
       "      <td>nặng_tình</td>\n",
       "      <td>A</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.170494</td>\n",
       "      <td>38.338900</td>\n",
       "      <td>19.600983</td>\n",
       "      <td>-1.993741</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>người_làm</td>\n",
       "      <td>người_bị_hại</td>\n",
       "      <td>N</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.135008</td>\n",
       "      <td>27.805490</td>\n",
       "      <td>18.980925</td>\n",
       "      <td>-3.418888</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>chần_chừ</td>\n",
       "      <td>lảo_đảo</td>\n",
       "      <td>V</td>\n",
       "      <td>3.20</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.112939</td>\n",
       "      <td>20.632544</td>\n",
       "      <td>18.305506</td>\n",
       "      <td>5.200466</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>khử_trùng</td>\n",
       "      <td>sát_trùng</td>\n",
       "      <td>V</td>\n",
       "      <td>5.53</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.705227</td>\n",
       "      <td>190.563691</td>\n",
       "      <td>13.240623</td>\n",
       "      <td>9.162421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word1         Word2 POS  Sim1  Sim2   STD  sim-cosine     sim-dot  \\\n",
       "0           biến          ngập   V  3.13  5.22  0.72   -0.004912   -1.493676   \n",
       "1    nhà_thi_đấu           nhà   N  3.07  5.12  1.18    0.082523   18.257401   \n",
       "2           động          tĩnh   V  0.60  1.00  0.95    0.277086   39.547434   \n",
       "3         khuyết            ưu   N  0.20  0.33  0.40    0.176799   40.841349   \n",
       "5       thủ_pháp     biện_pháp   N  4.13  6.88  1.26    0.402366  106.914893   \n",
       "..           ...           ...  ..   ...   ...   ...         ...         ...   \n",
       "393    triều_đại        cổ_đại   N  3.67  6.12  1.14    0.274376   76.308166   \n",
       "395    lình_xình     nặng_tình   A  1.33  2.22  1.14    0.170494   38.338900   \n",
       "396    người_làm  người_bị_hại   N  2.20  3.67  0.83    0.135008   27.805490   \n",
       "398     chần_chừ       lảo_đảo   V  3.20  5.33  0.98    0.112939   20.632544   \n",
       "399    khử_trùng     sát_trùng   V  5.53  9.22  0.81    0.705227  190.563691   \n",
       "\n",
       "     sim-euclid   sim-dice  sim-jaccard  \n",
       "0     25.296228   0.808093          1.0  \n",
       "1     22.118834  -3.074252          1.0  \n",
       "2     14.640360 -24.627852          1.0  \n",
       "3     19.508880  -0.468402          1.0  \n",
       "5     17.831482  -2.735911          1.0  \n",
       "..          ...        ...          ...  \n",
       "393   20.353694  -6.902056          1.0  \n",
       "395   19.600983  -1.993741          1.0  \n",
       "396   18.980925  -3.418888          1.0  \n",
       "398   18.305506   5.200466          1.0  \n",
       "399   13.240623   9.162421          1.0  \n",
       "\n",
       "[344 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = [('cosine', cosine), ('dot', dot), ('euclid', euclid),\n",
    "             ('dice', dice), ('jaccard', jaccard)]\n",
    "for name, function in distances:\n",
    "    simpairs[f'sim-{name}'] = simpairs.apply(sim, axis=1, sim_f=function)\n",
    "simpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pearson correlation coefficient:  (0.4468430791767022, 2.7457782359140626e-18)\n",
      " Spearman's rank correlation coefficient:  SpearmanrResult(correlation=0.4077422929392862, pvalue=3.2726552283981985e-15)\n"
     ]
    }
   ],
   "source": [
    "Sim1 = simpairs['Sim1'].tolist()\n",
    "Sim2 = simpairs['Sim2'].tolist()\n",
    "Cosine = simpairs['sim-cosine'].tolist()\n",
    "\n",
    "print(\" Pearson correlation coefficient: \", stats.pearsonr(Sim1, Cosine))\n",
    "print(\" Spearman's rank correlation coefficient: \", stats.spearmanr(Sim1, Cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning vocab: 100%|██████████| 77021/77021 [01:27<00:00, 881.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('đềnthờ', 0.5623567247142175), ('thápchuông', 0.5443984164585618), ('biatưởngniệm', 0.5406205813189373), ('giáođường', 0.5329588054503885), ('lăngmộ', 0.5287220564708057)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def topn(w: str,\n",
    "         vocab: List[str] = words,\n",
    "         encoder: Callable = word2vec,\n",
    "         distance_by: Callable = cosine,\n",
    "         n: int = 5) -> list:\n",
    "    input_encode = encoder(w)\n",
    "    vocab_sim = [(other, distance_by(input_encode, encoder(other)))\n",
    "                 for other in tqdm(vocab, \"Scanning vocab\")]\n",
    "    vocab_sim.sort(key=lambda x: x[1], reverse=True)\n",
    "    return vocab_sim[1:n+1]\n",
    "\n",
    "\n",
    "# task 2 with word: 'tượng_đài'. Note: words in vocabulary are all NORMALIZED!\n",
    "print(topn('tượng_đài', n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ! pip install tensorboard\n",
    "# ! pip install tensorflow\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 30\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ln1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.ln2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.Linear(hidden_dim+1, 2)\n",
    "\n",
    "    def _sim(self, x):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between pairs of embedding vector\n",
    "        :param x: input shape: [batch_size x 2 x embedding_dim]\n",
    "        :return: shape: [batch_size,] similarity between input pairs\n",
    "        \"\"\"\n",
    "        x1 = torch.squeeze(x[:, 0, ...], dim=1)  # -> [batch_size x embedding_dim]\n",
    "        x2 = torch.squeeze(x[:, 1, ...], dim=1)  # -> [batch_size x embedding_dim]\n",
    "        return F.cosine_similarity(x1, x2).reshape(-1,1)\n",
    "    def _diff(self, x):\n",
    "        \"\"\"\n",
    "        Calculate difference vector between pairs of embedding vector\n",
    "        :param x: input shape: [batch_size x 2 x embedding_dim]\n",
    "        :return: shape: [batch_size x embedding_dim]\n",
    "        \"\"\"\n",
    "        x1 = torch.squeeze(x[:, 0, ...], dim=1)  # -> [batch_size x embedding_dim]\n",
    "        x2 = torch.squeeze(x[:, 1, ...], dim=1)  # -> [batch_size x embedding_dim]\n",
    "        return x1-x2\n",
    "\n",
    "    def forward(self, x,**kwargs):\n",
    "        \"\"\"\n",
    "        :param x: input shape: [batch_size x 2 x embedding_dim]\n",
    "        :return: shape: [batch_size,] similarity between input pairs\n",
    "        \"\"\"\n",
    "        x = F.relu(self.ln1(x))\n",
    "        x = F.tanh(self.ln2(x))\n",
    "        x = F.relu(self.ln3(x))\n",
    "        x = F.tanh(self.ln4(x))\n",
    "        sim_ = self._sim(x)\n",
    "        dif_ = self._diff(x)\n",
    "        x = torch.cat((dif_, sim_),1) \n",
    "        x = self.ln5(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Discriminator(embedding_dim=150, hidden_dim=500)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "labels_dict = {\n",
    "    0: [1, 0],\n",
    "    1: [0, 1],\n",
    "}\n",
    "loss_f = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, (x, y) in enumerate(batchify(train_x, train_y, batch_size)):\n",
    "        y = torch.Tensor([labels_dict[yi] for yi in y])\n",
    "        optimizer.zero_grad()\n",
    "        predict_y = model(torch.Tensor(x), training=True)\n",
    "        loss_value = loss_f(predict_y, y)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss_value\n",
    "        # Gather data and report\n",
    "        running_loss += loss_value\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 1000  # loss per batch\n",
    "            print('epoch {}  batch {} loss: {}'.format(epoch_index, i + 1, last_loss))\n",
    "            running_loss = 0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  batch 100 loss: 0.1175815686583519\n",
      "epoch 0  batch 200 loss: 0.09047933667898178\n",
      "epoch 1  batch 100 loss: 0.08473016321659088\n",
      "epoch 1  batch 200 loss: 0.08031094819307327\n",
      "epoch 2  batch 100 loss: 0.08283901959657669\n",
      "epoch 2  batch 200 loss: 0.07886103540658951\n",
      "epoch 3  batch 100 loss: 0.08199287950992584\n",
      "epoch 3  batch 200 loss: 0.07765471935272217\n",
      "epoch 4  batch 100 loss: 0.08068524301052094\n",
      "epoch 4  batch 200 loss: 0.07579269260168076\n",
      "epoch 5  batch 100 loss: 0.07835826277732849\n",
      "epoch 5  batch 200 loss: 0.07256737351417542\n",
      "epoch 6  batch 100 loss: 0.07402992248535156\n",
      "epoch 6  batch 200 loss: 0.06711097061634064\n",
      "epoch 7  batch 100 loss: 0.06729568541049957\n",
      "epoch 7  batch 200 loss: 0.060301147401332855\n",
      "epoch 8  batch 100 loss: 0.05982448533177376\n",
      "epoch 8  batch 200 loss: 0.053859204053878784\n",
      "epoch 9  batch 100 loss: 0.05326998978853226\n",
      "epoch 9  batch 200 loss: 0.04836992919445038\n",
      "epoch 10  batch 100 loss: 0.048045191913843155\n",
      "epoch 10  batch 200 loss: 0.04367933049798012\n",
      "epoch 11  batch 100 loss: 0.043677639216184616\n",
      "epoch 11  batch 200 loss: 0.03952912241220474\n",
      "epoch 12  batch 100 loss: 0.03984959423542023\n",
      "epoch 12  batch 200 loss: 0.03578944876790047\n",
      "epoch 13  batch 100 loss: 0.03640991821885109\n",
      "epoch 13  batch 200 loss: 0.03240063413977623\n",
      "epoch 14  batch 100 loss: 0.03324122726917267\n",
      "epoch 14  batch 200 loss: 0.0293244868516922\n",
      "epoch 15  batch 100 loss: 0.030268430709838867\n",
      "epoch 15  batch 200 loss: 0.02651860937476158\n",
      "epoch 16  batch 100 loss: 0.027454718947410583\n",
      "epoch 16  batch 200 loss: 0.02393609657883644\n",
      "epoch 17  batch 100 loss: 0.02480914443731308\n",
      "epoch 17  batch 200 loss: 0.021540524438023567\n",
      "epoch 18  batch 100 loss: 0.02232116274535656\n",
      "epoch 18  batch 200 loss: 0.01931772008538246\n",
      "epoch 19  batch 100 loss: 0.02001446858048439\n",
      "epoch 19  batch 200 loss: 0.01727331057190895\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_one_epoch(epoch_index=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original test set (drop oov)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANT       0.99      0.91      0.95       466\n",
      "         SYN       0.90      0.99      0.95       380\n",
      "\n",
      "    accuracy                           0.95       846\n",
      "   macro avg       0.95      0.95      0.95       846\n",
      "weighted avg       0.95      0.95      0.95       846\n",
      "\n",
      "____________\n",
      "d-test set (drop oov, drop same pair with train set)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANT       1.00      0.91      0.95       466\n",
      "         SYN       0.90      1.00      0.95       369\n",
      "\n",
      "    accuracy                           0.95       835\n",
      "   macro avg       0.95      0.96      0.95       835\n",
      "weighted avg       0.96      0.95      0.95       835\n",
      "\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "predict = np.argmax(model(torch.Tensor(test_x)).detach().numpy(), axis=1).tolist()\n",
    "\n",
    "apredict = np.argmax(model(torch.Tensor(atest_x)).detach().numpy(), axis=1).tolist()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['ANT', 'SYN']\n",
    "print('original test set (drop oov)')\n",
    "print(classification_report(test_y.tolist(), predict, target_names=target_names))\n",
    "print('____________')\n",
    "print('d-test set (drop oov, drop same pair with train set)')\n",
    "print(classification_report(atest_y.tolist(), apredict, target_names=target_names))\n",
    "print('____________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 150])\n",
      "0\n",
      "ANT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_14624\\1117812591.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  input = torch.Tensor([[w1, w2]])\n",
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "def predict(w1: str, w2: str) -> str:\n",
    "    w1 = word2vec(word2norm(w1))\n",
    "    w2 = word2vec(word2norm(w2))\n",
    "    input = torch.Tensor([[w1, w2]])\n",
    "    print(input.shape)\n",
    "    output = model(input).detach().numpy()\n",
    "    output = np.argmax(output, axis=1).tolist()[0]\n",
    "    print(output)\n",
    "    if output == 0:\n",
    "        return 'ANT'\n",
    "    else:\n",
    "        return 'SYN'\n",
    "\n",
    "\n",
    "print(predict('thanh_danh', 'ô_nhục'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9ab610a55a5176458df2dff96f548f03c9f71ac1d536db2e7e5f1815100f41c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
